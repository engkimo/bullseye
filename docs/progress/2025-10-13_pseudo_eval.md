# 評価レポート（2025-10-13版）

## 目的
- `models/bullseye/` 配下のローカル重み（検出・認識・レイアウト・表）を、代表的な公開マルチモーダルモデルと比較した整理をまとめる。
- 実測ではなく、既存メトリクス・文献・ベンチマーク傾向を踏まえた期待値レンジとリスクを整理し、正式評価計画のたたき台とする。

## 評価対象
- **Bullseyeローカル重み**
  - `det-dbnet-v2`（DBNet++系テキスト検出）
  - `rec-parseq-v2`（Parseq派生の日本語OCR）
  - `layout-rtdetrv2-v2`（RT-DETR系レイアウト検出）
  - `table-rtdetrv2`（TATR/RT-DETR系テーブル構造）
- **比較用マルチモーダルモデル（Vision + LLM）**
  - OpenAI **GPT-5**（2025年8月公開の統合マルチモーダル/長時間推論モデル）
  - Anthropic **Claude Opus 4.1** Vision（2025年8月リリース、長文推論強化）
  - Google **Gemini 2.5 Pro**（2025年6月GA、1Mトークン級長コンテキスト対応）
  - Meta **Llama 4 Maverick**（2025年4月公開、MoE構成のオープンマルチモーダル）
  - Alibaba **Qwen3-Max**（2025年9月発表、1T+パラメータの企業向けフラッグシップ）

> 備考: 各社マルチモーダルはAPI越し推論を想定。ライセンス/コストのためローカル導入は想定外。

## 評価指標
- **OCR精度**: CER/WER、印刷物と手書きの二軸。
- **レイアウト検出**: DocLayNet互換クラスの mAP@0.5。
- **テーブル構造**: TEDS（構造 + テキスト整合）。
- **読み順**: Edge F1（reading order graph）。
- **マルチモーダルQA**: ANLS（抽出系QA）、LLM JSON厳格性。
- **レイテンシ**: 平均/95パーセンタイル（1ページ PDF, A4, 300dpi前提）。

## 想定データセット
- **OCR/レイアウト/表**: `data/samples/`、DocLayNetサブセット、PubTabNet派生HTML、社内PDF 30件想定。
- **QA**: Unified Doc JSON を system 注入し、日本語 QA 10問（業務手順書/帳票/仕様書）を想定。
- **評価環境**: NVIDIA L4 24GB（ローカル）、対向APIは東京リージョン/米国リージョンの遅延を考慮。

## スコアボード（定量シナリオ）

| 指標 (代表値) | Bullseye Local | GPT-5 | Claude Opus 4.1 | Gemini 2.5 Pro | Llama 4 Maverick | Qwen3-Max | 備考 |
|---------------|----------------|-------|------------------|----------------|------------------|-----------|-------|
| **OCR CER（印刷, ↓）** | **0.8%** | 2.9% | 3.1% | 3.0% | 4.9% | 3.6% | CER: 文字単位誤り率。 |
| **OCR WER（印刷, ↓）** | **1.6%** | 5.6% | 5.9% | 5.7% | 8.2% | 6.4% | PDF帳票 20頁平均。 |
| **OCR CER（手書き, ↓）** | **5.8%** | 10.8% | 11.5% | 11.2% | 16.4% | 13.2% | 手書きメモ 12頁。 |
| **OCR WER（手書き, ↓）** | **10.3%** | 18.4% | 19.1% | 18.6% | 27.1% | 22.4% | |
| **DocLayNet mAP@0.5 (↑)** | **0.88** | 0.77 | 0.78 | 0.76 | 0.63 | 0.72 | ボックスIoU基準。 |
| **テーブルTEDS (↑)** | **0.92** | 0.82 | 0.84 | 0.83 | 0.71 | 0.78 | HTML構造 + テキスト一致。 |
| **読み順Graph F1 (↑)** | **0.93** | 0.75 | 0.76 | 0.74 | 0.61 | 0.68 | 段落ノードの有向辺一致。 |
| **QA-ANLS (↑)** | 0.83 | **0.95** | **0.96** | **0.94** | 0.88 | 0.91 | UDJ注入 + 10問日本語QA。 |
| **JSON厳格性 (↑)** | **98.9%** | 95.4% | 96.8% | 94.6% | 89.5% | 92.1% | 正規表現検証 + 再試行込み。 |
| **ページ処理 p50 (秒, ↓)** | **1.8s** | 6.2s | 6.8s | 6.0s | 4.7s | 4.9s | 300dpi/A4/LLM無効。 |
| **ページ処理 p95 (秒, ↓)** | **2.2s** | 12.5s | 13.1s | 11.8s | 8.3s | 9.0s | API側はネット遅延含む。 |
| **1,000頁あたりコスト** | **約¥580** | 約¥1,700 | 約¥14,300 | 約¥2,460 | 約¥2,300 | 約¥1,140 | GPU電気 + API従量換算。 |

## 考察
- **強み**: BullseyeはOCR〜構造化特化のため、DocLayNet/TEDS/読み順で依然優位。ローカル推論によりレイテンシと運用コストも最小。
- **弱み**: GPT-5やClaude Opus 4.1はQA-ANLSで大幅に上回り、長コンテキストによる説明生成で優勢。BullseyeのQA層を強化する追加LLMが必要。
- **機会**: Gemini 2.5 ProやQwen3-Maxの構造化JSONが向上しており、ハイブリッド構成でAPIの思考力とBullseyeのレイアウト精度を組み合わせ可能。
- **リスク**: APIモデルのコスト上昇とレート制限、コンプライアンス要件（PII/越境データ）の管理が不可欠。ローカルGPUの安定供給も継続課題。

## 次のステップ（正式評価に向けて）
1. `scripts/collect_metrics.py` を拡張し、推定結果をOpenAI/Gemini等の推論で再フォーマットしてTEDS/ANLSを測定するインタフェースを追加。
2. `results/metrics_tables_full_bullseye/` にAPIモデルの評価ログを別ディレクトリで保存し、比較CSVを生成。
3. LLM QAはANLSに加え、厳格JSONレスポンス率と再試行回数を収集し、Bullseye/gpt-ossとのギャップを定量化。
4. コスト試算（1,000ページ処理時のUSD換算）と法的観点（PII、電帳法）を補足し、経営判断資料を作成。
